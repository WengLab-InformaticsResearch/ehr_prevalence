{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "from time import time\n",
    "import pickle\n",
    "from collections import namedtuple, Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats\n",
    "import cohd_temporal as cohd\n",
    "import importlib\n",
    "from cohd_temporal import AgeCounter, DeltaCounter  # Needed to load pickled cads and deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/path/to/data/dir'\n",
    "dir_data_out = '/path/to/data/out/dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: use the sql load query to insert the dataset_id so that we don't have to redundantly write out the dataset_id in each row of these files  \n",
    "  \n",
    "Sort the data in the same order as the primary keys / indexes for their destination SQL tables for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load concept counts\n",
    "with open(path.join(dir_data, 'concept_counts_perturbed.pkl'), 'rb') as f:\n",
    "    concept_counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the concept IDs\n",
    "concepts_sorted = sorted([k for k in concept_counts.keys()])\n",
    "\n",
    "# Write out as csv\n",
    "with open(path.join(dir_data_out, 'concept_counts.csv'), 'w') as f:\n",
    "    f.write('concept_id\\tconcept_count\\n')\n",
    "    for concept_id in concepts_sorted:\n",
    "        f.write(f'{concept_id}\\t{concept_counts[concept_id]}\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concept pair counts\n",
    "with open(path.join(dir_data, 'concept_pair_counts_perturbed.pkl'), 'rb') as f:\n",
    "    concept_pair_counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the concept pairs\n",
    "concept_pairs_sorted = sorted([k for k in concept_pair_counts.keys()])\n",
    "\n",
    "# Write out concept pair counts to csv\n",
    "with open(path.join(dir_data_out, 'concept_pair_counts.csv'), 'w') as f:\n",
    "    f.write('concept_id_1\\tconcept_id_2\\tconcept_count\\n')\n",
    "    for pair in concept_pairs_sorted:\n",
    "        f.write(f'{pair[0]}\\t{pair[1]}\\t{concept_pair_counts[pair]}\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grouped age distributions\n",
    "with open(path.join(dir_data, 'concept_age_group_dists_perturbed.pkl'), 'rb') as f:\n",
    "    gacs = pickle.load(f)\n",
    "\n",
    "# Sort the concept IDs\n",
    "concepts_sorted = sorted([k for k in gacs.keys()])\n",
    "\n",
    "file_age = path.join(dir_data_out, 'age_distributions.csv')\n",
    "file_age_scheme = path.join(dir_data_out, 'age_schemes.csv')\n",
    "with open(file_age, 'w') as f_age, open(file_age_scheme, 'w') as f_age_schemes:\n",
    "    \n",
    "    # Write headers\n",
    "    f_age.write('concept_id\\tbin\\tconcept_count\\n')\n",
    "    f_age_schemes.write('concept_id\\tbin_width\\tbins\\n')\n",
    "        \n",
    "    for concept_id in concepts_sorted:\n",
    "        gac = gacs[concept_id]\n",
    "        counts = gac.counts\n",
    "        \n",
    "        # Write the scheme\n",
    "        f_age_schemes.write(f'{concept_id}\\t{gac.bin_width}\\t{gac.bins}\\n')\n",
    "        \n",
    "        for i, c in enumerate(counts):\n",
    "            f_age.write(f'{concept_id}\\t{i}\\t{c}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grouped delta counts\n",
    "with open(path.join(dir_data, 'deltas_group_perturbed.pkl'), 'rb') as f:\n",
    "    gdcs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the concept pairs to make the load into SQL database faster\n",
    "concept_pairs_sorted = sorted([k for k in gdcs.keys()])\n",
    "\n",
    "# Write out concept pair counts to csv\n",
    "file_deltas = path.join(dir_data_out, 'deltas.csv')\n",
    "file_delta_schemes = path.join(dir_data_out, 'delta_schemes.csv')\n",
    "with open(file_deltas, 'w') as f_deltas, open(file_delta_schemes, 'w') as f_schemes:\n",
    "    f_deltas.write('concept_id_1\\tconcept_id_2\\tbin\\tconcept_count\\n')\n",
    "    f_schemes.write('concept_id_1\\tconcept_id_2\\tbin_width\\tn\\n')\n",
    "    for pair in concept_pairs_sorted:\n",
    "        gdc = gdcs[pair]\n",
    "        counts = gdc.counts\n",
    "        n = gdc.n\n",
    "        f_schemes.write(f'{pair[0]}\\t{pair[1]}\\t{gdc.bin_width}\\t{gdc.n}\\n')\n",
    "        for i, c in enumerate(counts):\n",
    "            f_deltas.write(f'{pair[0]}\\t{pair[1]}\\t{i-gdc.n}\\t{c}\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
