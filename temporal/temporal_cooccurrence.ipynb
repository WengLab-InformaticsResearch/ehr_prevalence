{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import cohd_temporal as cohd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/path/to/data/dir'\n",
    "file_pcs = path.join(dir_data, 'patient_code_sequences.txt')\n",
    "file_persons = path.join(dir_data, 'persons.csv')\n",
    "file_concepts = path.join(dir_data, 'concepts.csv')\n",
    "file_ingredients = path.join(dir_data, 'drug_ingredients.csv')\n",
    "file_condition_ancestors = path.join(dir_data, 'condition_ancestors.csv')\n",
    "file_cads = path.join(dir_data, 'concept_age_dists.pkl')\n",
    "file_cagds = path.join(dir_data, 'concept_age_group_dists.pkl')\n",
    "file_deltas = path.join(dir_data, 'deltas.pkl')\n",
    "file_backup_suffix = '.backup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column indices for persons.csv\n",
    "df_persons = pd.read_csv(file_persons, sep='\\t', header=0, index_col=0, \n",
    "                         parse_dates=['birth_date'], infer_datetime_format=True)\n",
    "\n",
    "# Check the data types of the columns\n",
    "df_persons.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concept definitions\n",
    "df_concepts = pd.read_csv(file_concepts, sep='\\t', header=0, index_col='concept_id')\n",
    "df_concepts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestor_map = defaultdict(list)\n",
    "\n",
    "# Read the in the mappings from drugs to ingredients\n",
    "df_ingredients = pd.read_csv(file_ingredients, sep='\\t', header=0)\n",
    "print(df_ingredients.dtypes)\n",
    "\n",
    "# Create a map from the observed drug_concept_id to the ingredient\n",
    "for index, row in df_ingredients.iterrows():\n",
    "    ancestor_map[int(row['drug_concept_id'])].append(int(row['ancestor_concept_id']))\n",
    "    \n",
    "# Read in the mappings for conditions\n",
    "df_condition_ancestors = pd.read_csv(file_condition_ancestors, sep='\\t', header=0)\n",
    "print(df_condition_ancestors.dtypes)\n",
    "\n",
    "# Add the conditions to the ancestor map\n",
    "for index, row in df_condition_ancestors.iterrows():\n",
    "    ancestor_map[int(row['condition_concept_id'])].append(int(row['ancestor_concept_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate age distributions for each concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for reading in the patient_code_sequences.txt\n",
    "\n",
    "Occurrence = namedtuple('Occurrence', ['concept_id', 'date'])\n",
    "\n",
    "\n",
    "def _process_occurrence_str(x):\n",
    "    concept_id_str, date_str = x.split(',')\n",
    "    occ = Occurrence(int(concept_id_str), datetime.strptime(date_str.strip(), '%Y-%m-%d'))\n",
    "    return occ\n",
    "\n",
    "\n",
    "def _trim_low_occurrences(occurrences, cads):\n",
    "    \"\"\" Removes occurrences if they're not in cads (i.e., count <= 10) \n",
    "    \n",
    "    Removes occurrences from the list passed in, does not create a new list \"\"\"\n",
    "    for i in range(len(occurrences)-1, -1, -1):\n",
    "        if occurrences[i].concept_id not in cads:\n",
    "            del occurrences[i]\n",
    "\n",
    "\n",
    "def _expand_occurrence_ancestors(occurrences, ancestor_map):\n",
    "    \"\"\" Expand a list of occurrences to include the ancestors defined in ancestor_map \n",
    "    Only adds the ancestor the first it occurs, i.e., if two concepts have the same ancestor,\n",
    "    the ancestor is only added the first time \"\"\"\n",
    "    occurrences_expanded = list()\n",
    "    observed_concepts = list()\n",
    "    for occurrence in occurrences:\n",
    "        concept_id = occurrence.concept_id\n",
    "        dt = occurrence.date\n",
    "        \n",
    "        # If we've already seen this concept (it must have been an ancestor of a previous concept), \n",
    "        # then move onto the next occurrence in the list\n",
    "        if concept_id in observed_concepts:\n",
    "            continue\n",
    "            \n",
    "        # First time we're seeing this concept, add it to the lists\n",
    "        occurrences_expanded.append(occurrence)\n",
    "        observed_concepts.append(concept_id)\n",
    "        \n",
    "        # Now add its ancestors\n",
    "        if concept_id in ancestor_map:\n",
    "            ancestors = ancestor_map[concept_id]\n",
    "            for anc in ancestors:\n",
    "                # Add ancestors that haven't already been observed\n",
    "                if anc not in observed_concepts:\n",
    "                    occurrences_expanded.append(Occurrence(anc, dt))\n",
    "                    observed_concepts.append(anc)\n",
    "                    \n",
    "    return occurrences_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_age(date, dob):\n",
    "    \"\"\" Gets the age of the person (years) on a date \"\"\"\n",
    "    # if the date occurs before the date of birth, return 0\n",
    "    if date < dob:\n",
    "        return 0\n",
    "    \n",
    "    years = date.year - dob.year\n",
    "    if date.month < dob.month or (date.month == dob.month and date.day < dob.day):\n",
    "        years -= 1\n",
    "    return years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_intermediates = False\n",
    "\n",
    "# For keeping track of processing time\n",
    "t1 = time()\n",
    "\n",
    "# cads - concept age distributions: holds the age (year) of first occurrence of each concept\n",
    "# keys are the concept_id (int)\n",
    "cads = defaultdict(AgeCounter)\n",
    "# cads = defaultdict(Counter)\n",
    "\n",
    "count = 0\n",
    "file_cads_backup = file_cads + file_backup_suffix\n",
    "\n",
    "# Read patient_code_sequences.txt\n",
    "with open(file_pcs) as fh:  \n",
    "    for line in fh:\n",
    "        split = line.strip().split('\\t')\n",
    "        \n",
    "        # person_id is the first entry\n",
    "        pid = int(split.pop(0))\n",
    "        \n",
    "        # Get the person's date of birth\n",
    "        dob = df_persons.loc[pid, 'birth_date']\n",
    "        \n",
    "        # Process the remaining string into a list of Occurrences\n",
    "        occurrences = [_process_occurrence_str(x) for x in split]\n",
    "        \n",
    "        # Expand the list of occurrences with the first instance of their ancestors\n",
    "        occurrences = _expand_occurrence_ancestors(occurrences, ancestor_map)\n",
    "        n_occurrences = len(occurrences)               \n",
    "        \n",
    "        # Track a distribution of ages for each concept\n",
    "        for occ in occurrences:\n",
    "            age = _get_age(occ.date, dob)\n",
    "            concept_id = occ.concept_id\n",
    "            cads[concept_id].add_age(age)\n",
    "        \n",
    "        # Display progress\n",
    "        count += 1\n",
    "        if count % 100000 == 0:\n",
    "            # Processing time and size of data structure\n",
    "            ellapsed_time = (time() - t1) / 60\n",
    "            print(f'{count} - {ellapsed_time:.01f} min')\n",
    "            \n",
    "            # Save a backup copy of the data\n",
    "            if save_intermediates:\n",
    "                with open(file_cads_backup, 'wb') as f:\n",
    "                    pickle.dump(cads, f)    \n",
    "                \n",
    "# Find concept-age distributions with total concept count <= 10 for deletion\n",
    "concepts_to_remove = list()\n",
    "for concept, cad in cads.items():\n",
    "    total_count = np.sum(cad.counts)\n",
    "    if (total_count) <= 10:\n",
    "        # Can't delete from the list while iterating, so keep track of concepts to delete\n",
    "        concepts_to_remove.append(concept)\n",
    "        \n",
    "# Delete low-count concepts from cads\n",
    "for concept in concepts_to_remove:\n",
    "    del cads[concept]\n",
    "del concepts_to_remove\n",
    "\n",
    "# Save the concept age distributions            \n",
    "with open(file_cads, 'wb') as f:\n",
    "    pickle.dump(cads, f)\n",
    "\n",
    "# Delete the backup file\n",
    "if save_intermediates and path.exists(file_cads_backup):\n",
    "    os.remove(file_cads_backup)\n",
    "\n",
    "# Display overall processing time\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'{count} - {ellapsed_time:.01f} min')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the ages together adaptively to reduce bins with small counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concept age-group distributions\n",
    "cagds = dict()\n",
    "concept_counts = dict()\n",
    "low_count = 9\n",
    "low_count_marker = 1\n",
    "t1 = time()\n",
    "\n",
    "for i, (c, age_counter) in enumerate(cads.items()):    \n",
    "    # Get the total concept count, perturbed\n",
    "    concept_counts[c] = cohd.poisson_perturbation(np.sum(age_counter.counts), n_samples=9)\n",
    "    \n",
    "    # Get the grouped age distribution\n",
    "    cagd = cohd.AdaptiveGroupedAgeCount(age_counter, low_count=low_count, lost_percent_limit=0.02, max_age=90)\n",
    "    \n",
    "    # AdaptiveGroupedAgeCount returns None if a suitable grouping wasn't found\n",
    "    if cagd is not None:\n",
    "        # Keep track of which bins to suppress\n",
    "        low_count_bins = (cagd.counts <= low_count) & (cagd.counts > 0)\n",
    "        \n",
    "        # Perturbation on all bins\n",
    "        cagd.counts = cohd.poisson_perturbation(cagd.counts, n_samples=9)\n",
    "\n",
    "        # Replace low-count bins with suppression marker\n",
    "        cagd.counts[low_count_bins] = low_count_marker\n",
    "\n",
    "        cagds[c] = cagd\n",
    "        \n",
    "    if i % 10000 == 0:\n",
    "        ellapsed_time = (time() - t1) / 60\n",
    "        print(f'{i}: {ellapsed_time}min')\n",
    "        \n",
    "# Save the concept age distributions        \n",
    "with open(path.join(dir_data, 'concept_age_group_dists_perturbed.pkl'), 'wb') as f:\n",
    "    pickle.dump(cagds, f)\n",
    "    \n",
    "# Save the concept counts\n",
    "with open(path.join(dir_data, 'concept_counts_perturbed.pkl'), 'wb') as f:\n",
    "    pickle.dump(concept_counts, f)\n",
    "    \n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'{i}: {ellapsed_time}min')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate time deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaCounter():\n",
    "    \"\"\"Class for keeping counts where the bins grow as power of 2\n",
    "    \n",
    "    For example, with n=5, the bins will be\n",
    "    <=-16 | -15 — -8 | -7 — -4 | -3 — -2 | -1 | 0 | 1 | 2 — 3 | 4 — 7 | 8 — 15 | >= 16\n",
    "    \"\"\"\n",
    "    # Maximum supported n\n",
    "    _max_n = 14\n",
    "    \n",
    "    # Lookup array for quickly finding out the index offset into the counts list    \n",
    "    _offset = [0] + [(int(np.log2(x)) + 1) for x in range(1, 2**(_max_n - 1))]    \n",
    "    \n",
    "    def __init__(self, n=13):\n",
    "        assert n >= 0\n",
    "        self.n = min(n, DeltaCounter._max_n)\n",
    "        self.bins = self.n * 2 + 1  # Number of bins\n",
    "        \n",
    "        # The delta at the start of the last bin. delta >= max_delta all go into the last bin\n",
    "        self.max_delta = 2**(self.n - 1)  \n",
    "        \n",
    "        # counts will be indexed from -n:1:n\n",
    "        self.counts = [0] * self.bins\n",
    "        \n",
    "    def add(self, delta):\n",
    "        if delta == 0:\n",
    "            self.counts[self.n] += 1\n",
    "            return\n",
    "        \n",
    "        # Determine the index into counts from the delta\n",
    "        if delta < 0:\n",
    "            if delta <= -self.max_delta:\n",
    "                index = 0\n",
    "            else:\n",
    "                index = self.n - min(DeltaCounter._offset[-delta], self.n)\n",
    "        else:\n",
    "            if delta >= self.max_delta:\n",
    "                index = self.bins - 1\n",
    "            else:\n",
    "                index = self.n + min(DeltaCounter._offset[delta], self.n)\n",
    "                \n",
    "        self.counts[index] += 1\n",
    "        \n",
    "    def bin_labels(self):\n",
    "        labels = [''] * self.n + ['0'] + [''] * self.n\n",
    "        \n",
    "        if self.n > 0:\n",
    "            labels[0] = f'-{2**(self.n-1)}+'\n",
    "            labels[self.bins - 1] = f'{2**(self.n-1)}+'\n",
    "            labels[slice(self.n - 1, self.n + 2, 2)] = ['-1', '1']\n",
    "        \n",
    "        for i in range(2, self.n):\n",
    "            lower = 2**(i-1)\n",
    "            upper = 2**i - 1\n",
    "            labels[self.n + i] = f'{lower} — {upper}'\n",
    "            labels[self.n - i] = f'-{upper} — -{lower}'\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def bin_labels_mixed(self):\n",
    "        \"\"\" Compact labels with mixed units (easier to interpret) \"\"\"\n",
    "        labels = ['0d', '1d', '2d', '4d', '8d', '16d', '32d', '64d', '128d', '256d', '1.4y', '2.8y', '5.6y', '11.2y', '22.4y', '44.9y', '89.7y', '179y', '359y']\n",
    "        labels = ['0d']\n",
    "        \n",
    "        for i in range(0, min(9, self.n)):\n",
    "            label = f'{2**i}d'\n",
    "            labels.append(label)\n",
    "            labels.insert(0, '-' + label)\n",
    "            \n",
    "        \n",
    "        for i in range(9, self.n):\n",
    "            label = f'{2**i/365.25:0.1f}y'\n",
    "            labels.append(label)\n",
    "            labels.insert(0, '-' + label)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def x(self):\n",
    "        \"\"\" X-ticks ranging from -self.n:self.n\"\"\"\n",
    "        return np.arange(-self.n, self.n+1)\n",
    "    \n",
    "    def normalized_counts(self):\n",
    "        \"\"\" Normalize the counts relative to the number of days in the bin \"\"\"\n",
    "        norm = 2**np.maximum(abs(self.x())-1, 0)\n",
    "        return self.counts / norm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_intermediates = False\n",
    "file_backup = file_deltas + file_backup_suffix\n",
    "\n",
    "# For keeping track of processing time\n",
    "t1 = time()\n",
    "\n",
    "# deltas - holds time deltas between pairs of concepts. \n",
    "# Indexed by tuple (concept1, concept2) where c1 < c2\n",
    "deltas = defaultdict(DeltaCounter)  \n",
    "\n",
    "count = 0\n",
    "\n",
    "with open(file_pcs) as fh:\n",
    "    for line in fh:\n",
    "        split = line.strip().split('\\t')\n",
    "        \n",
    "        # person_id is the first entry\n",
    "        pid = int(split.pop(0))\n",
    "        \n",
    "        # process the remaining string into a list of occurrence tuples\n",
    "        occurrences = [_process_occurrence_str(x) for x in split]\n",
    "        \n",
    "        # Don't process concepts whose single concept counts are too low\n",
    "        _trim_low_occurrences(occurrences, cads)\n",
    "        \n",
    "        # Expand the list of occurrences with the first instance of their ancestors\n",
    "        occurrences = _expand_occurrence_ancestors(occurrences, ancestor_map)\n",
    "        n_occurrences = len(occurrences)\n",
    "        \n",
    "        # calculate deltas from each pair of tuples\n",
    "        for i in range(n_occurrences - 1):\n",
    "            concept_i = occurrences[i].concept_id\n",
    "            concept_i_date = occurrences[i].date\n",
    "            assert concept_i\n",
    "            \n",
    "            for j in range(i + 1, n_occurrences):                \n",
    "                concept_j = occurrences[j].concept_id\n",
    "                delta = occurrences[j].date - concept_i_date\n",
    "                \n",
    "                # don't allow concept_id 0, and two concepts should not be the same\n",
    "                assert concept_j and concept_i != concept_j\n",
    "                \n",
    "                # place the smaller concept_id as the first concept in the tuple\n",
    "                if concept_i < concept_j:\n",
    "                    deltas[(concept_i, concept_j)].add(delta.days)\n",
    "                else:\n",
    "                    deltas[(concept_j, concept_i)].add(-delta.days)\n",
    "        \n",
    "        # display progress\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            ellapsed_time = (time() - t1) / 60\n",
    "            print(f'{count} - {ellapsed_time:.01f} min')\n",
    "            \n",
    "            # Save a backup copy of the data\n",
    "            if save_intermediates and count % 500000 == 0:\n",
    "                with open(file_backup, 'wb') as f:\n",
    "                    pickle.dump(deltas, f)       \n",
    "\n",
    "# Delete deltas with total co-occurrence <= 10\n",
    "pairs_to_remove = list()\n",
    "for concept_pair, delta in deltas.items():\n",
    "    total_pair_count = np.sum(delta.counts)\n",
    "    if (total_pair_count) <= 10:\n",
    "        # Can't delete from dict while iterating, so keep track of keys to delete\n",
    "        pairs_to_remove.append(concept_pair)\n",
    "        \n",
    "# Delete the identified keys and the list of keys\n",
    "for concept_pair in pairs_to_remove:\n",
    "    del deltas[concept_pair]\n",
    "del pairs_to_remove\n",
    "                    \n",
    "# Display overall processing time\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'{count} - {ellapsed_time:.01f} min')            \n",
    "            \n",
    "# Save the concept age distributions            \n",
    "with open(file_deltas, 'wb') as f:\n",
    "    pickle.dump(deltas, f)\n",
    "\n",
    "# Delete the backup file\n",
    "if save_intermediates and path.exists(file_backup):\n",
    "    os.remove(file_backup)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the delta bins together adaptively to reduce bins with small counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "gdcs = dict()\n",
    "concept_pair_counts = dict()\n",
    "low_count = 9\n",
    "low_count_marker = 1\n",
    "for i, (pair, delta) in enumerate(deltas.items()):\n",
    "    # Get the total concept pair count, perturbed\n",
    "    concept_pair_counts[pair] = cohd.poisson_perturbation(np.sum(delta.counts), n_samples=9)\n",
    "    \n",
    "    # Get grouped delta counts\n",
    "    gdc = cohd.AdaptiveGroupedDeltaCount2(delta, settings=[(1, 13), (2, 6), (4, 3), (8, 2), (16, 1)], \n",
    "                                          low_count=low_count, lost_percent_limit=0, verbose=False)    \n",
    "    \n",
    "    if gdc is not None:\n",
    "        # Keep track of which bins had low counts\n",
    "        low_count_bins = (gdc.counts <= low_count) & (gdc.counts > 0)\n",
    "        \n",
    "        # Poisson pertubation of all counts\n",
    "        gdc.counts = cohd.poisson_perturbation(gdc.counts, n_samples=9)\n",
    "        \n",
    "        # Replace the low count bins with the value indicating a low count\n",
    "        gdc.counts[low_count_bins] = low_count_marker\n",
    "        \n",
    "        gdcs[pair] = gdc\n",
    "    \n",
    "    if i % 100000 == 0:\n",
    "        ellapsed_time = (time() - t1) / 60\n",
    "        print(f'{i}: {ellapsed_time} min')\n",
    "    \n",
    "# Save the concept age distributions            \n",
    "with open(path.join(dir_data, 'concept_pair_counts_perturbed.pkl'), 'wb') as f:\n",
    "    pickle.dump(concept_pair_counts, f)\n",
    "    \n",
    "# Save the concept age distributions            \n",
    "with open(path.join(dir_data, 'deltas_group_perturbed.pkl'), 'wb') as f:\n",
    "    pickle.dump(gdcs, f)\n",
    "    \n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'ellapsed time: {ellapsed_time} min')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
